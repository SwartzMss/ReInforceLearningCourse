# 多臂老虎机问题示例说明文档

## 1. 问题背景

多臂老虎机问题（Multi-armed Bandit）是强化学习中的经典问题，主要解决 **探索（Exploration）** 与 **利用（Exploitation）** 之间的平衡问题。

- **探索**：尝试新的选项以获得更多信息。
- **利用**：利用已有信息选择当前最佳选项，从而获得较高奖励。

在实际问题中，每个“臂”（老虎机）的真实奖励分布未知，目标是在有限的试验次数内累计获得最大的奖励。

---

## 2. 算法原理：ε-贪婪策略

ε-贪婪策略是一种简单而有效的平衡探索和利用的方法，其主要思想如下：

- **随机探索**：以 ε 的概率随机选择一个臂，保证足够的探索。
- **贪婪利用**：以 1-ε 的概率选择当前奖励估计最高的臂，利用已获得的信息。

### 更新规则

每次获得奖励后，对被选择臂的奖励均值进行更新，更新公式采用增量更新方式：  
  **new_estimate = old_estimate + (reward - old_estimate) / count**

**参数说明：**

- **new_estimate**：更新后的奖励均值估计，即在本次尝试后得到的新估计值。
- **old_estimate**：更新前的奖励均值估计，即当前已知的旧估计值。
- **reward**：本次尝试获得的实际奖励值。
- **count**：该臂（老虎机）迄今为止被选择的总次数（包含当前这次）。这种方式无需保存所有历史奖励数据，而只需维护当前均值和选择次数。

---

## 3. 代码结构解析

整个示例代码主要分为三个部分：环境与算法封装、训练过程以及测试过程。

### 3.1 环境与算法封装

- **类 `EpsilonGreedyBandit`**

  - `__init__(true_means, epsilon)`：初始化真实均值、探索率、臂的数量、奖励估计以及计数器。
  - `select_arm()`：根据 ε-贪婪策略选择一个臂。如果生成的随机数小于 ε，则随机探索，否则选择当前估计奖励最高的臂。
  - `update(arm, reward)`：使用增量公式更新被选择臂的奖励估计。

### 3.2 训练过程

- **函数 `train_bandit(true_means, epsilon, iterations)`**

  训练过程中主要步骤：
  
  1. 创建一个 `EpsilonGreedyBandit` 对象。
  2. 在每次迭代中，根据策略选择一个臂。
  3. 模拟获得奖励（假设奖励服从正态分布，均值为对应臂的真实奖励，加上随机噪声）。
  4. 调用 `update` 方法更新该臂的奖励估计。
  5. 记录每次获得的奖励用于后续分析。

通过训练，算法逐步学会哪个臂的奖励较高，从而提高后续的收益。

### 3.3 测试过程

- **函数 `test_bandit(bandit, test_iterations)`**

  测试阶段采用固定的贪婪策略：
  
  1. 根据训练后的奖励估计选出最优臂（最佳老虎机）。
  2. 在测试阶段固定选择该臂，并模拟多次获得奖励。
  3. 计算测试阶段的平均奖励，用于评估训练效果。

这种方式可以验证训练后学习到的策略在固定选择最优臂时的表现。

---

## 4. 参数选择与效果展示

- **真实均值 (`true_means`)**  
  示例中定义了两个老虎机的真实均值，如 `[1.0, 1.8]`，代表不同老虎机的内在奖励水平。

- **探索率 (`epsilon`)**  
  训练阶段设置 `ε = 0.1`，即 10% 的概率进行探索。较高的 ε 值有助于初期充分探索，但长期来看可能导致过多的随机选择。

- **训练与测试迭代次数**  
  示例中训练阶段设定 1000 次尝试，测试阶段 500 次尝试。足够的训练次数有助于收敛，测试时多次实验可以平滑随机噪声的影响。

- **累计平均奖励曲线**  
  通过绘制训练过程中累计平均奖励随迭代次数的变化，可以观察算法的学习进展和收敛情况。

---
